# working through the following tutorial on LDA topic modeling
# https://www.tidytextmining.com/topicmodeling.html

library(topicmodels)
library(tm)
library(hunspell)

# Step 1: create a TDM object from my collection of tweets using the TM package (try going from tidy objects to TDMs and back)

# Step 1a: create a tidy object from the 'texts' df

# Sidebar: Use cleaning/preprocessing steps from this tweet-specific vignette: https://vsokolov.org/courses/41000/notes/trump-tweets.html

# Create "vectorised stemming/spellcheck function" using hunspell: 
my_hunspell_stem <- function(word) {
  stem_word <- hunspell_stem(word)[[1]]
  if (length(stem_word) == 0) return(word) else return(stem_word[1])
}
vec_hunspell_stem <- Vectorize(my_hunspell_stem, "word")

# Clean/preprocess tweets 
texts %>%
  mutate(text = str_replace_all(text, 
                                pattern=regex("(www|https?[^\\s]+)"),
                                replacement = "")) %>% #rm urls
  mutate(text = str_replace_all(text,
                                pattern = "[[:digit:]]",
                                replacement = "")) %>%
  unnest_tokens(word, text) %>%
  mutate(word = vec_hunspell_stem(word)) %>%
  anti_join(stop_words) %>% 
  count(element_id, word, sort = TRUE) %>%
  rename(document = element_id, term = word, count = n)  -> texts_td

# save to file 
save_as_csv(texts_td, file_name="data/tidy_tweets.csv")

# read back in from file
texts_td <- fread("data/tidy_tweets.csv", na.strings = c("",NA))

 
# Step 1b: create TDM using the cast function
texts_td %>%
  cast_dtm(document, term, count) -> texts_dtm

# Step 2: geneerate LDA model from dtm
texts_lda <- LDA(texts_dtm, k = 3, control = list(seed = 1234))

# Step 2a: tidy it up so you can see a table of probabilities per-word of being generated by each topic in your model

texts_topics <- tidy(texts_lda, matrix = "beta")

# Sidebar analysis: what are the top 10 most frequent terms per topic? 
texts_top_terms_per_topic <- texts_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# graph it & export
texts_top_terms_per_topic %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free" ) + 
  scale_y_reordered() # removes the ____ and topic number from y-axis

ggsave("output/top_terms_per_topic.png")

# conclusion: this method — like absolute word frequencies — kind of sucks even with cleaned data, because there's so much overlap in common terms between topics. 

# an altenative way to visualize the difference in topics is to graph the "beta spread" — which terms have the greatest difference in probability of appearing in Topic 1 vs. Topic 2 and vice versa? 

# The beta spread is calculated via log2(beta2/beta1).

beta_spread <- texts_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 0.001 | topic2 > 0.001 | topic3 > 0.001) %>%
  mutate(log_ratio2_1 = log2(topic2 / topic1)) %>%
  mutate(log_ratio3_1 = log2(topic3 / topic1)) %>%
  mutate(log_ratio3_2 = log2(topic3 / topic2))

# Next challenge - can you visualize the difference between topic 2 and topic 1, like fig. 6.3 in the textmining book? 

# get the top 10 & bottom 10, combine for graphing
top10_log_ration2_1 <- beta_spread %>%
  select(term, log_ratio2_1) %>%
  top_n(10, log_ratio2_1)

bottom10_log_ration2_1 <- beta_spread %>%
  select(term, log_ratio2_1) %>%
  top_n(-10, log_ratio2_1)

most_unique_by_topic <- rbind(top10_log_ration2_1, bottom10_log_ration2_1) %>%
  mutate(topic = ifelse(log_ratio2_1 > 0, "2","1"))

# graph it: 
plot <- ggplot(most_unique_by_topic)

xlim_min <- -10
xlim_max <- 10
  
plot + 
  geom_col(aes(x = log_ratio2_1, y = reorder(term, log_ratio2_1), fill = topic)) +
  xlim(xlim_min, xlim_max) +
  labs(title = "Beta Spread Between Topics 1 & 2", x = "Log2 Ratio", y = "") +
  theme(plot.title = element_text(hjust = 0.5)) # center title

  

